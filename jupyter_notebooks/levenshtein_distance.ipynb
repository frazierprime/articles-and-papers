{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as ddf\n",
    "import fletcher\n",
    "import jellyfish\n",
    "import numba\n",
    "import numpy\n",
    "import pandas\n",
    "import pyarrow\n",
    "from pathlib import Path\n",
    "\n",
    "this_data_root_path = '/Users/joshfrazier/data/kaggle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment out these lines if the file needs to be re-written for some reason.\n",
    "#tmp_dataframe = pandas.read_csv(f'{this_data_root_path}/test.csv')\n",
    "#tmp_dataframe.to_parquet(f'{this_data_root_path}/test.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2345796 entries, 0 to 2345795\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Dtype \n",
      "---  ------     ----- \n",
      " 0   question1  object\n",
      " 1   question2  object\n",
      "dtypes: object(2)\n",
      "memory usage: 527.9 MB\n"
     ]
    }
   ],
   "source": [
    "# Load the data back into memory (in this case, read: pandas). \n",
    "# This data is fairly large, and takes up about half a gig of RAM with the current default object type.\n",
    "dataframe = pandas.read_parquet(f'{this_data_root_path}/test.parquet')\n",
    "dataframe[['question1', 'question2']].info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NB: This method signature is tightly coupled to the training dataset in this tutorial.\n",
    "# If you wish to use it elsewhere, it should be generalized to take arbitrary datasets.\n",
    "def calculate_lev_distance_per_row(x, y):\n",
    "    if x is None or y is None:\n",
    "        return -1\n",
    "    else:\n",
    "        return jellyfish.levenshtein_distance(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This step is for demonstrative purposes, and is not necessary to run every time this notebook is run.\n",
    "%timeit dataframe.apply(lambda x: calculate_lev_distance_per_row(x['question1'], x['question2']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A straightforward way to utilize more CPUs on a local machine is to use dask and distributed\n",
    "# to spread the work among multiple cores.\n",
    "\n",
    "# Split the previous dataframe into even chunks.\n",
    "# THIS ONLY NEEDS TO BE DONE ONCE. UNCOMMENT TO WRITE THESE FILES.\n",
    "# chunked_dataframes = numpy.array_split(dataframe, 32)\n",
    "# for i, chunk in enumerate(chunked_dataframes):\n",
    "#     chunk.to_parquet(f'{this_data_root_path}/chunked/test-{i}.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2751378263910, 2345796)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start a local cluster and load the data into RAM.\n",
    "# This call to persist will only load the data once a computation reaches this point.\n",
    "# Here, we do a small, \"dummy\" computation, which ensures that our dataframe is in memory,\n",
    "# so that future benchmarks can be isolated to algorithm computation time.\n",
    "\n",
    "from dask.distributed import Client, LocalCluster, wait\n",
    "\n",
    "cluster = LocalCluster(dashboard_address=1000)\n",
    "client = Client(cluster)\n",
    "# Build a distributed dataframe.\n",
    "dask_dataframe = ddf.read_parquet(f'{this_data_root_path}/chunked/test-*.parquet', engine='pyarrow').persist()\n",
    "dask_dataframe['test_id'].sum().compute(), len(dask_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 481.41 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "2.17 s ± 5.23 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "# This time, run the same levenshtein calculation, but with distributed's future implementation.\n",
    "# As stated above, we are benchmarking the computation time ONLY, by this point, the dataframe is in memory.\n",
    "\n",
    "tasks = dask_dataframe.apply(lambda x: calculate_lev_distance_per_row(x['question1'], x['question2']), \n",
    "                             axis=1, \n",
    "                             meta=(None, 'int64'))\n",
    "result = client.compute(tasks)\n",
    "wait(result)\n",
    "\n",
    "# Cleanup\n",
    "result.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up after our previous cell.\n",
    "cluster.close()\n",
    "client.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2345796 entries, 0 to 2345795\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Dtype                      \n",
      "---  ------     -----                      \n",
      " 0   question1  fletcher_continuous[string]\n",
      " 1   question2  fletcher_continuous[string]\n",
      "dtypes: fletcher_continuous[string](2)\n",
      "memory usage: 287.4 MB\n"
     ]
    }
   ],
   "source": [
    "# So far, we've proven that in purely computational terms, that using jellyfish has resulted in a significant \n",
    "# performance boost, given its (jellyfish's) efficient C implementation. \n",
    "\n",
    "# The main areas for improvement where we will focus moving forward will be:\n",
    "# The overhead of using the PyObject structure.\n",
    "# The overhead of the string representations of the data.\n",
    "\n",
    "# In the case of strings, the PyObject overhead is much smaller than in the numeric case\n",
    "# since the payload data is much larger when compared to the size of the PyObject header.\n",
    "\n",
    "# Rather than loading the data using pandas directly, we'll use fletcher's wrapper method \"read_parquet\"\n",
    "# which will make the string columns available as pyarrow.Array-backed columns.\n",
    "\n",
    "fletcher_dataframe = fletcher.read_parquet(f'{this_data_root_path}/test.parquet', continuous=True)\n",
    "fletcher_dataframe[['question1', 'question2']].info(memory_usage='deep')\n",
    "\n",
    "# In our previous experiments, using Python's string encoding, the data took up 527.9 MB in RAM.\n",
    "# Arrow's string encoding utilizes a little over half the memory: 287.4 MB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our definition of Levenshtein distance is NOT based on the textbook definition, but is\n",
    "# instead based on a C implementation of the algorithm. See evernote and link to original\n",
    "# tutorial here: https://uwekorn.com/2020/12/08/levenshtein-distance-with-fletcher.html for the C-implementation.\n",
    "\n",
    "\"\"\"\n",
    "Compute levenshtein distance for a single row.\n",
    "\"\"\"\n",
    "# Is it necessary to pass the string length to this method? Why can't we just use len(stringX)?\n",
    "@numba.jit(nogil=True, nopython=True)\n",
    "def levenshtein_distance_by_row(stringOne, stringOneLength, stringTwo, stringTwoLength):\n",
    "    if stringOneLength > stringTwoLength:\n",
    "        stringOne, stringTwo = stringTwo, stringOne\n",
    "        stringOneLength, stringTwoLength = stringTwoLength, stringOneLength\n",
    "    \n",
    "    this_column = numpy.arange(stringOneLength + 1)\n",
    "    \n",
    "    for x in range(1, stringTwoLength + 1):\n",
    "        this_column[0] = x\n",
    "        last_diag = x - 1\n",
    "        \n",
    "        for y in range(1, stringOneLength + 1):\n",
    "            old_diag = this_column[y]\n",
    "            this_column[y] = min(this_column[y] + 1,\n",
    "                                min(this_column[y - 1] + 1, last_diag + (0 if stringOne[y - 1] == stringTwo[x - 1] else 1)),\n",
    "                                )\n",
    "            last_diag = old_diag\n",
    "    return this_column[stringOneLength]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import prange\n",
    "from fletcher.algorithms.string import _extract_string_buffers\n",
    "from fletcher._algorithms import _merge_valid_bitmaps\n",
    "\"\"\"\n",
    "Compute levenshtein distance for a whole column without nulls.\n",
    "\"\"\"\n",
    "@numba.jit(nogil=True, nopython=True, parallel=True)\n",
    "def levenshtein_numba_no_nulls(length, \n",
    "                              offsets_buffer_a, \n",
    "                              data_buffer_a, \n",
    "                              offsets_buffer_b, \n",
    "                              data_buffer_b, \n",
    "                              out):\n",
    "    for i in prange(length):\n",
    "        out[i] = levenshtein_distance_by_row(data_buffer_a[offsets_buffer_a[i] :],\n",
    "                                             offsets_buffer_a[i + 1] - offsets_buffer_a[i],\n",
    "                                             data_buffer_b[offsets_buffer_b[i] :],\n",
    "                                             offsets_buffer_b[i + 1] - offsets_buffer_b[i],)\n",
    "\n",
    "\"\"\"\n",
    "Compute levenshtein distance for a whole column where nulls occur.\n",
    "\"\"\"\n",
    "@numba.jit(nogil=True, nopython=True, parallel=True)\n",
    "def levenshtein_numba_nulls(length, \n",
    "                              valid, \n",
    "                              offsets_buffer_a, \n",
    "                              data_buffer_a, \n",
    "                              offsets_buffer_b, \n",
    "                              data_buffer_b, \n",
    "                              out):\n",
    "    for i in prange(length):\n",
    "        byte_offset = i // 8\n",
    "        bit_offset = i % 8\n",
    "        mask = numpy.uint8(1 << bit_offset)\n",
    "        is_valid = valid[byte_offset] & mask\n",
    "        \n",
    "        if is_valid:\n",
    "            out[i] = levenshtein_distance_by_row(data_buffer_a[offsets_buffer_a[i] :],\n",
    "                                                 offsets_buffer_a[i + 1] - offsets_buffer_a[i],\n",
    "                                                 data_buffer_b[offsets_buffer_b[i] :],\n",
    "                                                 offsets_buffer_b[i + 1] - offsets_buffer_b[i],)\n",
    "\n",
    "\"\"\"\n",
    "Compute the levenshtein distance for two fletcher.FletcherContinuousArray columns.\n",
    "\n",
    "This method delegates to the respective methods, based on whether or not they are robust to \n",
    "null values or the ommission of the validity bitmap.\n",
    "\"\"\"\n",
    "def calculate_levenshtein_distance_numba(a, b):\n",
    "    if len(a) != len(b):\n",
    "        raise ValueError('Arrays must be of equal lengths...')\n",
    "    \n",
    "    out = numpy.empty(len(a), dtype=int)\n",
    "    \n",
    "    offsets_buffer_a, data_buffer_a = _extract_string_buffers(a)\n",
    "    offsets_buffer_b, data_buffer_b = _extract_string_buffers(b)\n",
    "    \n",
    "    if a.null_count == 0 and b.null_count == 0:\n",
    "        levenstein_numba_no_nulls(len(a),\n",
    "                                offsets_buffer_a,\n",
    "                                data_buffer_a,\n",
    "                                offsets_buffer_b,\n",
    "                                data_buffer_b,\n",
    "                                 out,)\n",
    "        return pyarrow.array(out)\n",
    "    else:\n",
    "        valid = _merge_valid_bitmaps(a, b)\n",
    "        levenshtein_numba_nulls(len(a),\n",
    "                               valid,\n",
    "                               offsets_buffer_a,\n",
    "                               data_buffer_a,\n",
    "                               offsets_buffer_b,\n",
    "                               data_buffer_b,\n",
    "                               out,)\n",
    "        buffers = [pyarrow.py_buffer(x) for x in [valid, out]]\n",
    "        return pyarrow.Array.from_buffers(pyarrow.int64(), len(out), buffers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.78 s ± 104 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit levenshtein = calculate_levenshtein_distance_numba(fletcher_dataframe['question1'].values.data, fletcher_dataframe['question2'].values.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.83 s ± 125 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\"\"\"\n",
    "fletcher >= 0.7 introduces this convenience method, which removes the necessity to implement the methods\n",
    "- levenshtein_numba_no_nulls\n",
    "- levenshtein_numba_nulls\n",
    "- calculate_levenshtein_distance_numba\n",
    "\n",
    "and wraps them, and their non-parallelized variants, in this apply_binary_str method,\n",
    "which takes the dataframes to operate on, and the function to apply to each input.\n",
    "\"\"\"\n",
    "from fletcher.algorithms.string import apply_binary_str\n",
    "apply_binary_str(fletcher_dataframe['question1'].values.data,\n",
    "                      fletcher_dataframe['question2'].values.data,\n",
    "                      func=levenshtein_distance_by_row,\n",
    "                      output_dtype=numpy.int64,\n",
    "                      parallel=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "theAandP",
   "language": "python",
   "name": "theaandp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
